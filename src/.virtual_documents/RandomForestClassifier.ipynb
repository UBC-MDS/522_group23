import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from docopt import docopt
import os, os.path
import errno

from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    make_scorer,
    precision_score,
    recall_score,
    average_precision_score, 
    auc
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import (
    RandomizedSearchCV,
    cross_val_score,
    cross_validate,
    cross_val_predict
)
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

from plot_confusion_matrix import plot_confusion_mat
from get_valid_score import mean_std_cross_val_scores


path = "../data/processed"
train_df = pd.read_csv(path+"/bank-additional-train.csv")
test_df = pd.read_csv(path+"/bank-additional-test.csv")


X_train = train_df.drop(columns=["y"])
X_test = test_df.drop(columns=["y"])

y_train = train_df["y"]
y_test = test_df["y"]


train_df.apply(lambda x: pd.unique(x).tolist())


numeric_features = [
    "age",
    "duration",
    "campaign",
    "pdays",
    "previous",
    "emp.var.rate",
    "cons.price.idx",
    "cons.conf.idx",
    "euribor3m",
    "nr.employed",
]

categorical_features = [
    "job",
    "marital",
    "education",
    "default",
    "housing",
    "loan",
    "contact",
    "month",
    "day_of_week",
    "poutcome",
]

binary_features = [
    "y",
]

drop_features = []
target = "y"


assert len(numeric_features+categorical_features) == len(X_train.columns)


numeric_transformer = make_pipeline(
        SimpleImputer(strategy="median"),
        StandardScaler()
    )

categorical_transformer = make_pipeline(
    SimpleImputer(strategy="constant", fill_value="missing"),
    OneHotEncoder(handle_unknown="ignore", sparse=False),
)

preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features),
    (categorical_transformer, categorical_features),
    ("drop", drop_features),
)
preprocessor


results = {}


scoring_metrics = [
    "accuracy",
    "f1",
    "recall",
    "precision",
]


dummy = DummyClassifier()
pipe = make_pipeline(preprocessor, dummy)

results['dummy'] = mean_std_cross_val_scores(
    pipe, X_train, y_train, cv=5, return_train_score=True, scoring = scoring_metrics
)
pd.DataFrame(results).T


y_pred = cross_val_predict(pipe, X_train, y_train, cv=5)
plot_confusion_mat(y_train, y_pred, 'Dummy Classifier');


param_grid = { 
    'RFC__max_features' : ["auto", "sqrt", "log2"],
    'RFC__min_samples_split' : range(1, 100),
    'RFC__max_depth' : range(1,5000),
    'RFC__class_weight' : ["balanced", "balanced_subsample"],
    'RFC__ccp_alpha' : 10**np.arange(-3,3, dtype=float),
}

pipe = Pipeline([
    ('preprocessor',preprocessor), 
    ('RFC',RandomForestClassifier(random_state=123, n_jobs=-1))
])

random_search_RFC = RandomizedSearchCV(estimator=pipe,
                                       param_distributions=param_grid,
                                       n_iter = 20,
                                       n_jobs = -1,
                                       random_state = 123,
                                       return_train_score = True,
                                       scoring = scoring_metrics,
                                       refit = 'f1',
                                      )
random_search_RFC.fit(X_train, y_train);


print("Best hyperparameter values: ", random_search_RFC.best_params_)
print(f"Best f1 score: {random_search_RFC.best_score_:0.3f}")


best_RFC_CV_results = pd.DataFrame(random_search_RFC.cv_results_)[[
'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',
       'params',
       'mean_train_accuracy','std_train_accuracy',
       'mean_train_f1', 'std_train_f1',
       'mean_train_recall', 'std_train_recall',
       'mean_train_precision', 'std_train_precision',
       'mean_test_accuracy','std_test_accuracy', 'rank_test_accuracy',
       'mean_test_f1','std_test_f1', 'rank_test_f1', 
       'mean_test_recall', 'std_test_recall','rank_test_recall', 
       'mean_test_precision','std_test_precision', 'rank_test_precision',
]].set_index("rank_test_f1").sort_index()
best_RFC_CV_results


best_RFC_params = {key.replace('RFC__',''):val for (key, val) in random_search_RFC.best_params_.items()}
best_RFC_params['random_state']=123
best_RFC_params['n_jobs']=-1
best_RFC_params


best_RFC = pipe = Pipeline([
    ('preprocessor',preprocessor), 
    ('RFC',RandomForestClassifier(**best_RFC_params))
])

best_RFC.fit(X_train, y_train)
best_RFC.score(X_train, y_train)


y_pred = best_RFC.predict(X_train)
plot_confusion_mat(y_train, y_pred,'Random Forest on Train Data');


y_pred = best_RFC.predict(X_test)
plot_confusion_mat(y_test, y_pred,'Random Forest on Test Data');


param_grid = { 
    'LR__C' : np.linspace(1,50,100),
    'LR__class_weight' : ["balanced", None],
}

pipe_lr = Pipeline([
    ('preprocessor',preprocessor), 
    ('LR',LogisticRegression(max_iter=1000, random_state=123))
])

random_search_LR = RandomizedSearchCV(estimator=pipe_lr,
                                       param_distributions=param_grid,
                                       n_jobs = -1,
                                       random_state = 123,
                                       return_train_score = True,
                                       scoring = scoring_metrics,
                                       refit = 'f1',
                                      )
random_search_LR.fit(X_train, y_train);


print("Best hyperparameter values: ", random_search_LR.best_params_)
print(f"Best f1 score: {random_search_LR.best_score_:0.3f}")


best_LR_CV_results = pd.DataFrame(random_search_LR.cv_results_)[[
'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',
       'params',
       'mean_train_accuracy','std_train_accuracy',
       'mean_train_f1', 'std_train_f1',
       'mean_train_recall', 'std_train_recall',
       'mean_train_precision', 'std_train_precision',
       'mean_test_accuracy','std_test_accuracy', 'rank_test_accuracy',
       'mean_test_f1','std_test_f1', 'rank_test_f1', 
       'mean_test_recall', 'std_test_recall','rank_test_recall', 
       'mean_test_precision','std_test_precision', 'rank_test_precision',
]].set_index("rank_test_f1").sort_index()
best_LR_CV_results


best_lr_params = {key.replace('LR__',''):val for (key, val) in random_search_LR.best_params_.items()}
best_lr_params['random_state']=123
best_lr_params['max_iter']=1000
best_lr_params


best_lr = Pipeline([
    ('preprocessor',preprocessor), 
    ('LR',LogisticRegression(**best_lr_params))
])
best_lr.fit(X_train, y_train)
best_lr.score(X_train, y_train)


y_pred = best_lr.predict(X_train)
plot_confusion_mat(y_train, y_pred,'Logistics Regression on Train Data');


y_pred = best_lr.predict(X_test)
plot_confusion_mat(y_test, y_pred,'Logistics Regression on Test Data');


categorical_features_ohe = list(
    preprocessor.named_transformers_["pipeline-2"]
    .named_steps["onehotencoder"]
    .get_feature_names_out(categorical_features)
)
new_columns = (
    numeric_features + categorical_features_ohe
)


lr_coefs = pd.DataFrame(data=best_lr[1].coef_[0], index=new_columns, columns=["Coefficient"])
lr_coefs.sort_values(by="Coefficient", ascending=False).head(20)
