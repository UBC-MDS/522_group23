---
title: "Predicting customer response to Bank telemarketing campaign"
author: "Melisa Maidana, Steven Lio, Zheren Xu"
date: "11/26/2021"
output: 
  html_document:
      toc: true
bibliography: bank_marketing_refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(knitr)
library(kableExtra)
```

## Introduction

Telemarketing campaigns can be very expensive to institutions. 
The possibility to predict the likelihood of customer response can lead to more efficient strategies, 
that reduce implementation costs and maximize the success rate.

The objective of this project is to identify which customers are more likely 
to respond positively to a telemarketing campaign and subscribe to a new product (a long-term deposit). 
To address the predictive question posed above, we plan to conduct an exploratory data analysis 
to identify the best features that can help predict the customer response. 
Our objective is to build a machine learning model that can predict if a certain customer 
looks alike the target audience for this product.

## Data

The data set used in this project is related with direct marketing campaigns (phone calls) of a Portuguese banking institution [@moro2014data] can be found [here](http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip).

The data set contains 20 features, plus the desired target. 
Each row represents information of one client, including personal and banking attributes, 
and information related to the past interactions with the telemarketer.

The data set presents class imbalance, since only about 11% of the records are targeted as positive
(meaning that the customer responded to the telemarketing offer).

```{r load_eda_table, echo=FALSE, include = FALSE}
summary <- read_csv("../results/eda_summary_table.csv")
summary <- summary |>  # remove first column (index)
    select(-1) 
```

```{r eda_class_imbalance, echo=FALSE}
summary %>%
  kbl(caption = "Table 1. Summary of examples per customer response in the dataset. Class imbalance is observed.") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r eda_boxplot, fig.align = 'center', echo=FALSE, fig.cap="Figure 1. Distribution of customer response per age.", out.width = '60%'}
knitr::include_graphics("../results/eda_boxplot_by_age.png")
```

## Model building
For the studies, Random Forest Classifier and Logistic regression model were trained using 80% of the 
data, the 20% of the data were used for validation of the model performance. For each model, a 5-fold 
[cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is conducted and a randomized
search algorithm is used for hyper-parameter tuning for each model and f1-score is used for the scoring metric. 

The model is compared based on the Accuracy/f1-score/recall rate/precision.
- All process is done in Python 3.9 and the machine learning library [`scikit-learn`](https://scikit-learn.org/stable/) is used
for the processing and model building.

- Preprocessing of training data:
  - Data were randomized and split based on 80% for training and 20% for testing.
  - All features are used in the models and can be classified into categorical features and numeric features.
  - Missing numeric values are filled using median, while missing values in categorical features are filled as "missing".
  - All numeric features are scaled using the mean and variance (Standardization) to ensure that all features have the same weight before training.
  - Categorical features are encoded as dummy variables.
  
- Parameters and hyper-parameters tuning.
  - To speed up the training process and ensure the training process to be reproducible, the value of `n_jobs` is set to `-1` and `random_state` to `123`.
  - The hyper-parameters for both models are tuned by `RandomizedSearchCV` and the optimal parameter values are chosen based on f1-score.

- The baseline accuracy of the model is 88.8% (obtained from the Dummy Classifier using highest frequency).

### Random Forest Classifier:
The best hyper-parameters for the Random Forest are:
```{r Load_RFC_params, echo=FALSE, include=FALSE}
RFC_BestParams <- read_csv("../results/RFC_BestParams.csv")
RFC_BestParams <- RFC_BestParams %>% select(-1) 
```
```{r RFC_params, echo=FALSE}
RFC_BestParams %>%
  kbl(caption = "Table 2. Best hyper-parameters for Random Forest Classifier.") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r conmat_RFC, echo=FALSE, fig.align = 'center', fig.cap="Figure 2. Confusion matrix for Random Forest Classifier on Test data", out.width = '60%'}
knitr::include_graphics("../results/BestRandomForest_ConMat_Test.jpg")
```

```{r RFC_scores, echo=FALSE}
tibble(Accuracy="82.9%",
       Precision="39.7%",
       Recall="96.4%",
       f1="56.2%") %>%
  kbl(caption = "Table 3. The best model on predicting test data scores:") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```
The best parameters gives a decent scores on the prediction on test data. We do
see the model is not as precise but it can accuratly recall the success response.

### Logistic Regression:
```{r Load_LR_params, echo=FALSE, include=FALSE}
LR_BestParams <- read_csv("../results/LR_BestParams.csv")
LR_BestParams <- LR_BestParams %>% select(-1) 
```
```{r LR_params, echo=FALSE}
LR_BestParams %>%
  kbl(caption = "Table 4. Best hyper-parameters for Logistic Regression Classifier.") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r conmat_LR, echo=FALSE, fig.align = 'center', fig.cap="Figure 3. Confusion matrix for Logistic Regression on Test data", out.width = '60%'}
knitr::include_graphics("../results/BestLogisticRegression_ConMat_Test.jpg")
```

```{r LR_scores, echo=FALSE}
tibble(Accuracy="86.1%",
       Precision="44.6%",
       Recall="90.3%",
       f1="59.7%") %>%
  kbl(caption = "Table 5. The best model on predicting test data scores:") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```
Logistic Regression overall perform better than Random Forest Classifier and it
provides a good interpretable results as we assess the coefficient later hence it
is more preferable than Random Forest Classifier.


## Analysis and Results Discussion
From the results above, we conclude the Logistic Regression model will be 
the best for predicting customer response to the telemarketing campaign. 
Logistics Regression also provides the ability to interpret the different features
that drives the prediction when we assess the coefficient of the final model.


```{r load_coef_table, echo=FALSE, include = FALSE}
lr_coeff <- read_csv("../results/BestLogisticRegression_Coefficients.csv") %>% arrange(desc(Coefficient))
colnames(lr_coeff) <- c("Feature","Coefficient")
lr_coeff
```
```{r coeff_barplot, echo=FALSE, fig.align = 'center', fig.cap="Figure 4. Top 10 Coefficients from Logistics Regression Model", out.width = '60%'}
ggplot(lr_coeff[1:10,], aes(x=Coefficient, y=reorder(Feature, Coefficient), fill=Coefficient)) +
  geom_bar(stat="identity", show.legend = FALSE) +
  geom_text(aes(label=round(Coefficient,2)), hjust = 1.3, color="white", size=5.0) +
  theme_classic() +
  labs(x="Features",
       y="Coefficient Values",
       title="Top 10 Coefficients") +
  theme(plot.title=element_text(hjust=0.5, face="bold"),
        axis.title.x=element_text(face="bold"),
        axis.title.y=element_text(face="bold"))
```

We can see from the graph that `duration` and `month_mar` has the highest coefficient 
value in the final model. `duration` corresponding to the duration in seconds and 
`month_mar` is the date from last contact with the bank in March. Although both 
variables may not available for new customer or someone who never contacted the 
bank. But if a customer did contact the bank, which means he/she may open to be 
contacted by telemarketer. What is missing here may be the reason of last contact.
Since if the customer contact the bank is to complains then calling them again to 
sell product may not be a good idea. The reason why it is March is not known here, 
but maybe the program ran in March usually was really good. We also see that August
and October is also one of the best period for the campaign response.

It is interesting to see `cons.price.idx` and `poutcome_success` shows up as the 
3rd and 4th important variable. `cons.price.idx`(Consumer Price Index) a quarterly
updated variable indicates the changes in prices as experienced by consumers, which
can tells us when the changes is higher, customer may want to switch to a new product
for a better price. This may not be good for a revenue perspective since we may ended
up lowering our revenue per customer, but it will for sure help with customer retention.
`poutcome_success` indicates previous success with marketing campaign. Which make 
sense since if a customer accepted the offer he/she may accept a new one again.

To optimize effective telemarketing campaign, the bank can prioritize on those who 
contacted the bank previous, and those who accepted previous campaign offer but not
anyone. Those who accepted previous campaign offer for certain time ago and make sure
to further classify them into different revenue bucket to maximize campaign ROI.

March, August, October are the best period historically for high campaign success.
Also those who are retired (lot's of saving) and students (price conscious) are two
key groups to go after.

Overall the model has a very stable accuracy and recall rate. Hence with the correct
variables, the model can identify those who will likely to to response to the telemarketing
campaign. We also do see our best model have a higher False Positive rate hence it may
predicted some who may response but actually haven't. We can explore more variables
such as reason of last contact and the tenure of the customer with the bank. As 
new customer or those who complains a lot probably won't response to the campaign.

The other benefit about Logistic Regression is that we can further prioritize the 
customers by contacting those who receive a higher probability of will response until
we reach the campaign target and budget.

## References
